{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "understanding_linear_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3qIxs67dB8u",
        "colab_type": "text"
      },
      "source": [
        "_Lambda School Data Science — Linear Models_\n",
        "\n",
        "# Understanding Linear Regression\n",
        "\n",
        "#### Objectives\n",
        "- understand how ordinary least squares regression minimizes the sum of squared errors\n",
        "- understand how linear algebra can solve ordinary least squares regression\n",
        "- get and interpret coefficients of a linear model\n",
        "- visualize a line of best fit in 2D, and hyperplane in 3D\n",
        "- use regression metrics: MSE, RMSE, MAE, R^2\n",
        "\n",
        "#### Extra Links\n",
        "- [Statistics 101: Simple Linear Regression](https://www.youtube.com/watch?v=ZkjP5RJLQF4) (20 minute video)\n",
        "- [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf), Chapter 3.1, Simple Linear Regression, & 3.2, Multiple Linear Regression\n",
        "- Priceonomics, [The Discovery of Statistical Regression](https://priceonomics.com/the-discovery-of-statistical-regression/)\n",
        "- Priceonomics, [Why the Father of Modern Statistics Didn’t Believe Smoking Caused Cancer](https://priceonomics.com/why-the-father-of-modern-statistics-didnt-believe/)\n",
        "- Harvard Business Review, [When to Act on a Correlation, and When Not To](https://hbr.org/2014/03/when-to-act-on-a-correlation-and-when-not-to)\n",
        "- [xkcd 552: Correlation](https://www.explainxkcd.com/wiki/index.php/552:_Correlation)\n",
        "- [xkcd 1725: Linear Regression](https://www.explainxkcd.com/wiki/index.php/1725:_Linear_Regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN49dLAsrB9p",
        "colab_type": "text"
      },
      "source": [
        "## What is Linear Regression?\n",
        "\n",
        "[Linear Regression](https://en.wikipedia.org/wiki/Linear_regression) is a statistical model that seeks to describe the relationship between some y variable and one or more x variables. \n",
        "\n",
        "In the simplest case, linear regression seeks to fit a straight line through a cloud of points. This line is referred to as the \"regression line\" or \"line of best fit.\" This line tries to summarize the relationship between our X and Y in a way that enables us to use the equation for that line to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpMgFRGTq-Xi",
        "colab_type": "code",
        "outputId": "44fe9242-bd4c-4687-dde2-5d8d182e66bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "x = [10,  8, 13,  9, 11, 14,  6,  4, 12,  7,  5]\n",
        "y = [ 8,  6,  7,  8,  8,  9,  7,  4, 10,  4,  5]\n",
        "sns.regplot(x, y);"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndCIil2CgExG5BAOdiMgl/j+7/lbGKuBuvQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pizS3rWdB8v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "### Synonyms for \"y variable\" \n",
        "- Dependent Variable\n",
        "- Response Variable\n",
        "- Outcome Variable \n",
        "- Predicted Variable\n",
        "- Measured Variable\n",
        "- Explained Variable\n",
        "- Label\n",
        "- Target\n",
        "\n",
        "### Synonyms for \"x variable\"\n",
        "- Independent Variable\n",
        "- Explanatory Variable\n",
        "- Regressor\n",
        "- Covariate\n",
        "- Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkBmrFzcdB8v",
        "colab_type": "text"
      },
      "source": [
        "## Simple Linear Regresion\n",
        "\n",
        "#### Making Predictions\n",
        "\n",
        "Say that we were trying to create a model that captured the relationship between temperature outside and ice cream sales. In Machine Learning our goal is often different that of other flavors of Linear Regression Analysis, because we're trying to fit a model to this data with the intention of making **predictions** on new data (in the future) that we don't have yet.\n",
        "\n",
        "#### What are we trying to predict?\n",
        "\n",
        "So if we had measured ice cream sales and the temprature outside on 11 different days, at the end of our modeling **what would be the thing that we would want to predict? - Ice Cream Sales or Temperature?**\n",
        "\n",
        "We would probably want to be measuring temperature with the intention of using that to **forecast** ice cream sales. If we were able to successfully forecast ice cream sales from temperature, this might help us know beforehand how much ice cream to make or how many cones to buy or on which days to open our store, etc. Being able to make predictions accurately has a lot of business implications. This is why making accurate predictions is so valuable (And in large part is why data scientists are paid so well).\n",
        "\n",
        "#### y variable intuition\n",
        "\n",
        "We want the thing that we're trying to predict to serve as our **y** variable. This is why it's sometimes called the \"predicted variable.\" We call it the \"dependent\" variable because our prediction for how much ice cream we're going to sell \"depends\" on the temperature outside. \n",
        "\n",
        "#### x variable intuition\n",
        "\n",
        "All other variables that we use to predict our y variable (we're going to start off just using one) we call our **x** variables. These are called our \"independent\" variables because they don't *depend* on y, they \"explain\" y. Hence they are also referred to as our \"explanatory\" variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKjLA8aAdB8w",
        "colab_type": "text"
      },
      "source": [
        "## Example: Predict presidential election voting\n",
        "\n",
        "#### Douglas Hibbs, [Background Information on the ‘Bread and Peace’ Model of Voting in Postwar US Presidential Elections](https://douglas-hibbs.com/background-information-on-bread-and-peace-voting-in-us-presidential-elections/)\n",
        "\n",
        "> Aggregate two-party vote shares going to candidates of the party holding the presidency during the postwar era are well explained by just two fundamental determinants:\n",
        "\n",
        "> (1) Positively by weighted-average growth of per capita real disposable personal income over the term.  \n",
        "> (2) Negatively by cumulative US military fatalities (scaled to population) owing to unprovoked, hostile deployments of American armed forces in foreign wars.\n",
        "\n",
        "#### Data sources\n",
        "- 1952-2012: Douglas Hibbs, [2014 lecture at Deakin University Melbourne](http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf), Slide 40\n",
        "- 2016, Vote Share: [The American Presidency Project](https://www.presidency.ucsb.edu/statistics/elections)\n",
        "- 2016, Recent Growth in Personal Incomes: [The 2016 election economy: the \"Bread and Peace\" model final forecast](https://angrybearblog.com/2016/11/the-2016-election-economy-the-bread-and-peace-model-final-forecast.html)\n",
        "- 2016, US Military Fatalities: Assumption that Afghanistan War fatalities in 2012-16 occured at the same rate as 2008-12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ohXjHbUdB8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "columns = ['Year','Incumbent Party Candidate','Other Candidate','Incumbent Party Vote Share']\n",
        "\n",
        "data = [[1952,\"Stevenson\",\"Eisenhower\",44.6],\n",
        "        [1956,\"Eisenhower\",\"Stevenson\",57.76],\n",
        "        [1960,\"Nixon\",\"Kennedy\",49.91],\n",
        "        [1964,\"Johnson\",\"Goldwater\",61.34],\n",
        "        [1968,\"Humphrey\",\"Nixon\",49.60],\n",
        "        [1972,\"Nixon\",\"McGovern\",61.79],\n",
        "        [1976,\"Ford\",\"Carter\",48.95],\n",
        "        [1980,\"Carter\",\"Reagan\",44.70],\n",
        "        [1984,\"Reagan\",\"Mondale\",59.17],\n",
        "        [1988,\"Bush, Sr.\",\"Dukakis\",53.94],\n",
        "        [1992,\"Bush, Sr.\",\"Clinton\",46.55],\n",
        "        [1996,\"Clinton\",\"Dole\",54.74],\n",
        "        [2000,\"Gore\",\"Bush, Jr.\",50.27],\n",
        "        [2004,\"Bush, Jr.\",\"Kerry\",51.24],\n",
        "        [2008,\"McCain\",\"Obama\",46.32],\n",
        "        [2012,\"Obama\",\"Romney\",52.00], \n",
        "        [2016,\"Clinton\",\"Trump\",48.2]]\n",
        "        \n",
        "votes = pd.DataFrame(data=data, columns=columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R679kBPdB8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = ['Year','Average Recent Growth in Personal Incomes']\n",
        "\n",
        "data = [[1952,2.40],\n",
        "        [1956,2.89],\n",
        "        [1960, .85],\n",
        "        [1964,4.21],\n",
        "        [1968,3.02],\n",
        "        [1972,3.62],\n",
        "        [1976,1.08],\n",
        "        [1980,-.39],\n",
        "        [1984,3.86],\n",
        "        [1988,2.27],\n",
        "        [1992, .38],\n",
        "        [1996,1.04],\n",
        "        [2000,2.36],\n",
        "        [2004,1.72],\n",
        "        [2008, .10],\n",
        "        [2012, .95], \n",
        "        [2016, .10]]\n",
        "        \n",
        "growth = pd.DataFrame(data=data, columns=columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5654kSpOdB81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Fatalities denotes the cumulative number of American military\n",
        "fatalities per millions of US population the in Korea, Vietnam,\n",
        "Iraq and Afghanistan wars during the presidential terms\n",
        "preceding the 1952, 1964, 1968, 1976 and 2004, 2008 and\n",
        "2012 elections.\n",
        "\n",
        "http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf\n",
        "\"\"\"\n",
        "\n",
        "columns = ['Year','US Military Fatalities per Million']\n",
        "\n",
        "data = [[1952,190],\n",
        "        [1956,  0],\n",
        "        [1960,  0],\n",
        "        [1964,  1],\n",
        "        [1968,146],\n",
        "        [1972,  0],\n",
        "        [1976,  2],\n",
        "        [1980,  0],\n",
        "        [1984,  0],\n",
        "        [1988,  0],\n",
        "        [1992,  0],\n",
        "        [1996,  0],\n",
        "        [2000,  0],\n",
        "        [2004,  4],\n",
        "        [2008, 14],\n",
        "        [2012,  5], \n",
        "        [2016,  5]]\n",
        "        \n",
        "deaths = pd.DataFrame(data=data, columns=columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyneMpsAdB9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = votes.merge(growth).merge(deaths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsslMxfHdB9N",
        "colab_type": "text"
      },
      "source": [
        "### Plot univariate correlations\n",
        "[Seaborn tutorial: Visualizing linear relationships](https://seaborn.pydata.org/tutorial/regression.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QDlIwG-mdB9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "target = 'Incumbent Party Vote Share'\n",
        "features = ['Average Recent Growth in Personal Incomes', \n",
        "            'US Military Fatalities per Million']\n",
        "\n",
        "for feature in features:\n",
        "    sns.lmplot(x=feature, y=target, data=df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnt0FvBddB9Q",
        "colab_type": "text"
      },
      "source": [
        "We can see from the scatterplot that these data points seem to follow a somewhat linear relationship for the \"Average Recent Growth in Personal Incomes\" feature. This means that we could probably summarize their relationship well by fitting a line of best fit to these points. Lets do it.\n",
        "\n",
        "\n",
        "## The Equation for a Line\n",
        "\n",
        "A common equation for a line is:\n",
        "\n",
        "\\begin{align}\n",
        "y = mx + b\n",
        "\\end{align}\n",
        "\n",
        "Where $m$ is the slope of our line and $b$ is the y-intercept. \n",
        "\n",
        "If we want to plot a line through our cloud of points we figure out what these two values should be. Linear Regression seeks to **estimate** the slope and intercept values that describe a line that best fits the data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2j_yV-sdB9R",
        "colab_type": "text"
      },
      "source": [
        "## The Anatomy of Linear Regression\n",
        "\n",
        "- Intercept: The $b$ value in our line equation $y=mx+b$\n",
        "- Slope: The $m$ value in our line equation $y=mx+b$. These two values together define our regression line.\n",
        "- $\\hat{y}$ : A prediction\n",
        "- Line of Best Fit (Regression Line)\n",
        "- Predicted (fitted) Values: Points on our regression line\n",
        "- Observed Values: Points from our dataset\n",
        "- Error: The distance between predicted and observed values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwELnPXZFzt8",
        "colab_type": "text"
      },
      "source": [
        "Ordinary Least Squares Regression is a way to solve for $m$ and $b$.\n",
        "\n",
        "Let's start by seeing what would happen if we just guessed and checked some values for $m$ and $b$. \n",
        "\n",
        "What's the line of \"best\" fit look like? What's the error?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv-goEV4dB9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoFgNKrpdB9T",
        "colab_type": "text"
      },
      "source": [
        "## R Squared:  $R^2$\n",
        "\n",
        "One final attribute of linear regressions that we're going to talk about today is a measure of goodness of fit known as $R^2$ or R-squared. $R^2$ is a statistical measure of how close the data are fitted to our regression line. A helpful interpretation for the $R^2$ is the percentage of the dependent variable that is explained by the model.\n",
        "\n",
        "In other words, the $R^2$ is the percentage of y that is explained by the x variables included in the model. For this reason the $R^2$ is also known as the \"coefficient of determination,\" because it explains how much of y is explained (or determined) by our x varaibles. We won't go into the calculation of $R^2$ today, just know that a higher $R^2$ percentage is nearly always better and indicates a model that fits the data more closely. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEnpH_RDdB9U",
        "colab_type": "text"
      },
      "source": [
        "## Residual Error \n",
        "\n",
        "The residual error is the distance between points in our dataset and our regression line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmnoUUHKdB9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regression_residuals(df, feature, target, m, b):\n",
        "    x = df[feature]\n",
        "    y = df[target]\n",
        "    y_pred = m*x + b\n",
        "    \n",
        "    plt.scatter(x, y, label='y_true')\n",
        "    plt.plot(x, y_pred, label='y_pred')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Plot residual errors\n",
        "    for x, y1, y2 in zip(x, y, y_pred):\n",
        "        plt.plot((x, x), (y1, y2), color='grey')\n",
        "        \n",
        "    mae = mean_absolute_error(y, y_pred) \n",
        "    r2 = r2_score(y, y_pred)\n",
        "    print('Mean Absolute Error:', mae)\n",
        "    print('R^2:', r2)\n",
        "        \n",
        "regression_residuals(df, feature='Average Recent Growth in Personal Incomes', \n",
        "                     target='Incumbent Party Vote Share', m=3, b=46)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HedRkOBEdB9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ipywidgets import interact, fixed\n",
        "\n",
        "interact(regression_residuals, \n",
        "         df=fixed(df), \n",
        "         feature=fixed('Average Recent Growth in Personal Incomes'), \n",
        "         target=fixed('Incumbent Party Vote Share'), \n",
        "         m=(-10,10,0.5), \n",
        "         b=(40,60,0.5));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKWKFpcidB9Y",
        "colab_type": "text"
      },
      "source": [
        "## More Formal Notation\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1436/1*_TqRJ9SmwFzRigJhMiN2uw.png\" width=\"600\">\n",
        "\n",
        "We have talked about a line of regression being represented like a regular line $y=mx+b$ but as we get to more complicated versions we're going to need to extend this equation. So lets establish the proper terminology.\n",
        "\n",
        "**X** - Independent Variable, predictor variable, explanatory variable, regressor, covariate\n",
        "\n",
        "**Y** - Response variable, predicted variable, measured vairable, explained variable, outcome variable\n",
        "\n",
        "$\\beta_0$ - \"Beta Naught\" or \"Beta Zero\", the intercept value. This is how much of y would exist if X were zero. This is sometimes represented by the letter \"a\" but I hate that. So it's \"Beta 0\" during my lecture.\n",
        "\n",
        "$\\beta_1$ - \"Beta One\" The primary coefficient of interest. This values is the slope of the line that is estimated by \"minimizing the sum of the squared errors/residuals\" - We'll get to that. \n",
        "\n",
        "$\\epsilon$ - \"Epsilon\" The \"error term\", random noise, things outside of our model that affect y."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlhU7ewgdB9Y",
        "colab_type": "text"
      },
      "source": [
        "## Minimizing the Sum of the Squared Error\n",
        "\n",
        "The most common method of estimating our $\\beta$ parameters  is what's known as \"Ordinary Least Squares\" (OLS). (There are different methods of arriving at a line of best fit). OLS estimates the parameters that minimize the squared distance between each point in our dataset and our line of best fit. \n",
        "\n",
        "\\begin{align}\n",
        "SSE = \\sum(y_i - \\hat{y})^2\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWOfefPGdB9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.patches import Rectangle\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def regression_squared_errors(df, feature, target, m, b):\n",
        "    x = df[feature]\n",
        "    y = df[target]\n",
        "    y_pred = m*x + b\n",
        "    \n",
        "    fig = plt.figure(figsize=(7,7))\n",
        "    ax = plt.axes()\n",
        "    ax.scatter(x, y, label='y_true')\n",
        "    ax.plot(x, y_pred, label='y_pred')\n",
        "    ax.legend()\n",
        "    \n",
        "    # Plot square errors\n",
        "    xmin, xmax = ax.get_xlim()\n",
        "    ymin, ymax = ax.get_ylim()\n",
        "    scale = (xmax-xmin)/(ymax-ymin)\n",
        "    for x, y1, y2 in zip(x, y, y_pred):\n",
        "        bottom_left = (x, min(y1, y2))\n",
        "        height = abs(y1 - y2)\n",
        "        width = height * scale\n",
        "        ax.add_patch(Rectangle(xy=bottom_left, width=width, height=height, alpha=0.1))\n",
        "        \n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y, y_pred)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    print('Mean Squared Error:', mse)\n",
        "    print('Root Mean Squared Error:', rmse)\n",
        "    print('Mean Absolute Error:', mae)\n",
        "    print('R^2:', r2)\n",
        "    \n",
        "regression_squared_errors(df, feature='Average Recent Growth in Personal Incomes', \n",
        "           target='Incumbent Party Vote Share', m=3, b=46)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY9Wvv-YdB9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interact(regression_squared_errors, \n",
        "         df=fixed(df), \n",
        "         feature=fixed('Average Recent Growth in Personal Incomes'), \n",
        "         target=fixed('Incumbent Party Vote Share'), \n",
        "         m=(-10,10,0.5), \n",
        "         b=(40,60,0.5));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JENtcq0qdB9d",
        "colab_type": "text"
      },
      "source": [
        "## Hypotheses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyGDRdXkdB9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = 46\n",
        "ms = np.arange(-10,10,0.5)\n",
        "sses = []\n",
        "feature = 'Average Recent Growth in Personal Incomes'\n",
        "\n",
        "for m in ms:\n",
        "    predictions = m * df[feature] + b\n",
        "    errors = predictions - df[target]\n",
        "    square_errors = errors ** 2\n",
        "    sse = square_errors.sum()\n",
        "    sses.append(sse)\n",
        "    \n",
        "hypotheses = pd.DataFrame({'Slope': ms})\n",
        "hypotheses['Intercept'] = b\n",
        "hypotheses['Sum of Square Errors'] = sses\n",
        "\n",
        "hypotheses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_6w2NijdB9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hypotheses.plot(x='Slope', y='Sum of Square Errors', \n",
        "                title=f'Intercept={b}');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79P6zF_KdB9h",
        "colab_type": "text"
      },
      "source": [
        "## Scikit-learn\n",
        "\n",
        "#### Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Scikit-Learn's Estimator API\n",
        "\n",
        "> Most commonly, the steps in using the Scikit-Learn estimator API are as follows (we will step through a handful of detailed examples in the sections that follow).\n",
        "\n",
        "> 1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn. \n",
        "> 2. Choose model hyperparameters by instantiating this class with desired values. \n",
        "> 3. Arrange data into a features matrix and target vector following the discussion above.\n",
        "> 4. Fit the model to your data by calling the `fit()` method of the model instance.\n",
        "> 5. Apply the Model to new data: For supervised learning, often we predict labels for unknown data using the `predict()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnLrLQ1adB9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5biGqP0dB9i",
        "colab_type": "text"
      },
      "source": [
        "## Linear Algebra!\n",
        "\n",
        "The same result that is found by minimizing the sum of the squared errors can be also found through a linear algebra process known as the \"Least Squares Solution:\"\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{\\beta} = (X^{T}X)^{-1}X^{T}y\n",
        "\\end{align}\n",
        "\n",
        "Before we can work with this equation in its linear algebra form we have to understand how to set up the matrices that are involved in this equation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEBgV_xqdB9j",
        "colab_type": "text"
      },
      "source": [
        "### The $\\beta$ vector\n",
        "\n",
        "The $\\beta$ vector represents all the parameters that we are trying to estimate, our $y$ vector and $X$ matrix values are full of data from our dataset. The $\\beta$ vector holds the variables that we are solving for: $\\beta_0$ and $\\beta_1$\n",
        "\n",
        "Now that we have all of the necessary parts we can set them up in the following equation:\n",
        "\n",
        "\\begin{align}\n",
        "y = X \\beta + \\epsilon\n",
        "\\end{align}\n",
        "\n",
        "Since our $\\epsilon$ value represents **random** error we can assume that it will equal zero on average.\n",
        "\n",
        "\\begin{align}\n",
        "y = X \\beta\n",
        "\\end{align}\n",
        "\n",
        "The objective now is to isolate the $\\beta$ matrix. We can do this by pre-multiplying both sides by \"X transpose\" $X^{T}$.\n",
        "\n",
        "\\begin{align}\n",
        "X^{T}y =  X^{T}X \\beta\n",
        "\\end{align}\n",
        "\n",
        "Since anything times its transpose will result in a square matrix, if that matrix is then an invertible matrix, then we should be able to multiply both sides by its inverse to remove it from the right hand side. (We'll talk tomorrow about situations that could lead to $X^{T}X$ not being invertible.)\n",
        "\n",
        "\\begin{align}\n",
        "(X^{T}X)^{-1}X^{T}y =  (X^{T}X)^{-1}X^{T}X \\beta\n",
        "\\end{align}\n",
        "\n",
        "Since any matrix multiplied by its inverse results in the identity matrix, and anything multiplied by the identity matrix is itself, we are left with only $\\beta$ on the right hand side:\n",
        "\n",
        "\\begin{align}\n",
        "(X^{T}X)^{-1}X^{T}y = \\hat{\\beta}\n",
        "\\end{align}\n",
        "\n",
        "We will now call it \"beta hat\" $\\hat{\\beta}$ because it now represents our estimated values for $\\beta_0$ and $\\beta_1$\n",
        "\n",
        "### Lets calculate our $\\beta$ coefficients with numpy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPoxzXtlvCGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.api import add_constant\n",
        "\n",
        "X = add_constant(df[feature].values)\n",
        "print('X')\n",
        "print(X)\n",
        "\n",
        "y = df[target].values[:, np.newaxis]\n",
        "print('y')\n",
        "print(y)\n",
        "\n",
        "X_transpose = X.T\n",
        "print('X Transpose')\n",
        "print(X_transpose)\n",
        "\n",
        "X_transpose_X = X_transpose @ X\n",
        "print('X Transpose X')\n",
        "print(X_transpose_X)\n",
        "\n",
        "X_transpose_X_inverse = np.linalg.inv(X_transpose_X)\n",
        "print('X Transpose X Inverse')\n",
        "print(X_transpose_X_inverse)\n",
        "\n",
        "X_transpose_y = X_transpose @ y\n",
        "print('X Transpose y')\n",
        "print(X_transpose_y)\n",
        "\n",
        "beta_hat = X_transpose_X_inverse @ X_transpose_y\n",
        "print('Beta Hat')\n",
        "print(beta_hat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i18xwsVxdB9l",
        "colab_type": "text"
      },
      "source": [
        "# Multiple Regression\n",
        "\n",
        "Simple or bivariate linear regression involves a single $x$ variable and a single $y$ variable. However, we can have many $x$ variables. A linear regression model that involves multiple x variables is known as **Multiple** Regression (NOT MULTIVARIATE).\n",
        "\n",
        "\\begin{align}\n",
        "y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8zmTC-12-K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcHvzlcfOIg8",
        "colab_type": "text"
      },
      "source": [
        "## Visualize hyperplane of best fit in 3D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aWyg1DWyph3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stackoverflow.com/a/47230966\n",
        "# Plotly notebook mode with google colaboratory\n",
        "# You need to define this function\n",
        "# And call it in each offline plotting cell\n",
        "\n",
        "def configure_plotly_browser_state():\n",
        "    import IPython\n",
        "    display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msewYJ85_16k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "def viz3D(fitted_model, X, features, target='', num=100):\n",
        "    \"\"\"\n",
        "    Visualize model predictions in 3D, for regression model fit on 2 features\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    fitted_model : scikit-learn model, already fitted\n",
        "    X : pandas dataframe, which was used to fit model\n",
        "    features : list of strings, column names of the 2 features\n",
        "    target : string, name of target\n",
        "    num : int, number of grid points for each feature\n",
        "    \n",
        "    References\n",
        "    ----------\n",
        "    https://plot.ly/python/3d-charts/\n",
        "    \"\"\"\n",
        "    feature1, feature2 = features\n",
        "    min1, max1 = X[feature1].min(), X[feature1].max()\n",
        "    min2, max2 = X[feature2].min(), X[feature2].max()\n",
        "    x1 = np.linspace(min1, max1, num)\n",
        "    x2 = np.linspace(min2, max2, num)\n",
        "    combos = list(itertools.product(x1, x2))\n",
        "    Z = fitted_model.predict(combos).reshape(num, num)\n",
        "    \n",
        "    configure_plotly_browser_state()\n",
        "    data = [go.Surface(x=x1, y=x2, z=Z)]\n",
        "    layout = go.Layout(\n",
        "        scene={'xaxis': {'title': feature1, 'range': [min1,max1], 'showticklabels': True}, \n",
        "               'yaxis': {'title': feature2, 'range': [min2,max2], 'showticklabels': True}, \n",
        "               'zaxis': {'title': target, 'showticklabels': True}}, \n",
        "    )\n",
        "    fig = go.Figure(data=data, layout=layout)\n",
        "    iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRoZIBc33GeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueHbXPJ4dB9p",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality in Linear Regression\n",
        "\n",
        "Muliple Regression is simply an extension of the bivariate case. The reason why we see the bivariate case demonstrated so often is simply because it's easier to graph and all of the intuition from the bivariate case is the same as we keep on adding explanatory variables.\n",
        "\n",
        "As we increase the number of $x$ values in our model we are simply fitting a n-1-dimensional plane to an n-dimensional cloud of points within an n-dimensional hypercube. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N12KLVsdB9q",
        "colab_type": "text"
      },
      "source": [
        "## Interpreting Coefficients\n",
        "\n",
        "One of Linear Regression's strengths is that the parameters of the model (coefficients) are readily interpretable and useful. Not only do they describe the relationship between x and y but they put a number on just how much x is associated with y. We should be careful to not speak about this relationshiop in terms of causality because these coefficients are in fact correlative measures. We would need a host of additional techniques in order to estimate a causal effect using linear regression (econometrics).\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{\\beta} = \\frac{Cov(x,y)}{Var(y)}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TTvvbn4dB9v",
        "colab_type": "text"
      },
      "source": [
        "## Why is Linear Regression so Important?\n",
        "\n",
        "### Popularity \n",
        "\n",
        "Linear Regression is an extremely popular technique that every data scientist **needs** to understand. It's not the most advanced technique and there are supervised learning techniques that will obtain a higher accuracy, but where it lacks in accuracy it makes up for it in interpretability and simplicity.\n",
        "\n",
        "### Interpretability\n",
        "\n",
        "Few other models possess coefficients that are so directly linked to their variables with a such a clear interpretation. Tomorrow we're going to learn about ways to make them even easier to interpret.\n",
        "\n",
        "### Simplicity\n",
        "\n",
        "A linear regression model can be communicated just by writing out its equation. It's kind of incredible that such high dimensional relationships can be described from just a linear combination of variables and coefficients. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxUj5y_3JFzw",
        "colab_type": "text"
      },
      "source": [
        "# Assignment\n",
        "- Continue to predict New York City apartment rents. This is your last assignment with this dataset.\n",
        "- You may select any number of features. You are encouraged to engineer new features.\n",
        "- Get and plot your model's coefficients.\n",
        "- Report your Root Mean Squared Error, Mean Absolute Error, and R^2 Score, for your Train and Test sets. Share your scores with your cohort on Slack!\n",
        "- Fit a model with 2 features, and visualize the plane of best fit in 3D.\n",
        "- Commit your notebook to your fork of the repo.\n",
        "\n",
        "## Stretch Goals\n",
        "\n",
        "Study more about Linear Regression. Here are two helpful links. If you find more links, share your favorites with your cohort on Slack.\n",
        "\n",
        "1. Watch this 20 minute video that just hit 1 million views: Brandon Foltz, Statistics 101: Simple Linear Regression (https://www.youtube.com/watch?v=ZkjP5RJLQF4)\n",
        "2. Skim _An Introduction to Statistical Learning_, Chapter 3.1, Simple Linear Regression, & Chapter 3.2, Multiple Linear Regression (http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf)\n",
        "\n",
        "In your 3D visualization, can you include the actual datapoints, like in [this notebook](https://nbviewer.jupyter.org/urls/s3.amazonaws.com/datarobotblog/notebooks/multiple_regression_in_python.ipynb)? Can you also include the residual lines from the datapoints to the plane of the best fit, like in _An Introduction to Statistical Learning?_ This would be hard to do, but awesome!\n",
        "\n",
        "\n",
        "Can you get creative with feature engineering? Share with your cohort on Slack. We mentioned some feature ideas at the end of last lesson, but didn't demonstrate how to engineer them. So here are some example solutions:\n",
        "\n",
        "```python\n",
        "# Does apartment have a non-empty description?\n",
        "df['description'] = df['description'].str.strip().fillna('')\n",
        "df['has_description'] = df['description'] != ''\n",
        "\n",
        "# How long is the description?\n",
        "df['description_length'] = df['description'].str.len()\n",
        "\n",
        "# How many total perks does each apartment have?\n",
        "perk_cols = ['elevator', 'cats_allowed', 'hardwood_floors', 'dogs_allowed',\n",
        "             'doorman', 'dishwasher', 'no_fee', 'laundry_in_building',\n",
        "             'fitness_center', 'pre-war', 'laundry_in_unit', 'roof_deck',\n",
        "             'outdoor_space', 'dining_room', 'high_speed_internet', 'balcony',\n",
        "             'swimming_pool', 'new_construction', 'exclusive', 'terrace', \n",
        "             'loft', 'garden_patio', 'common_outdoor_space', \n",
        "             'wheelchair_access']\n",
        "df['perk_count'] = df[perk_cols].sum(axis=1)\n",
        "\n",
        "# Are pets allowed?\n",
        "df['pets_allowed'] = (df['cats_allowed']==1) | (df['dogs_allowed']==1)\n",
        "```\n"
      ]
    }
  ]
}